# Transformer ENâ†’ZH from Scratch

>åŒ—äº¬äº¤é€šå¤§å­¦ ã€ç ”ã€‘å¤§æ¨¡å‹åŸºç¡€ä¸åº”ç”¨

> ä¸€ä¸ªTransformer è‹±â†’ä¸­æœºå™¨ç¿»è¯‘å°é¡¹ç›®ï¼šè¦†ç›–è‡ªæ³¨æ„åŠ›ï¼ˆSelf-Attentionï¼‰ã€ç¼–ç å™¨â€”è§£ç å™¨ï¼ˆEncoderâ€“Decoderï¼‰ã€ä½ç½®ç¼–ç ã€é®ç½©ã€è®­ç»ƒ/è¯„ä¼°ä¸å¯è§†åŒ–ï¼›æä¾›å¯å¤ç°å®éªŒè„šæœ¬ä¸æŠ¥å‘Šæ¨¡æ¿ã€‚

- **ç³»ç»Ÿ**ï¼šUbuntu 20.04
- **Python**ï¼š3.11.13
- **CUDA / PyTorch**ï¼šCUDA 12.4 Â· PyTorch 2.5.1
- **ç¡¬ä»¶**ï¼šå•å¡ RTX 4090

## ğŸ“¦ ä¾èµ–å®‰è£…

```bash
conda create -n trans python=3.11.13 -y
conda activate trans
cd Transformer/
pip install -r requirements.txt
# æˆ–ï¼špip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 \
#   --index-url https://download.pytorch.org/whl/cu124
# pip install matplotlib tqdm torchsummary scikit-learn numpy pandas pyyaml
```

## ğŸ“ ä»“åº“ç›®å½•ç»“æ„

```bash
Transformer/
â”œâ”€ configs/
â”‚  â””â”€ config.yaml                 # å…¨å±€é…ç½®ï¼šæ¨¡å‹/è®­ç»ƒ/è·¯å¾„ç­‰
â”œâ”€ data/
â”‚  â””â”€ en-cn/
â”‚     â”œâ”€ cmn.txt                  # åŸå§‹åŒè¯­ï¼ˆè‹±æ–‡\ä¸­æ–‡ï¼‰
â”‚     â”œâ”€ english_sentences.txt    # é¢„å¤„ç†è‹±æ–‡
â”‚     â””â”€ chinese_sentences.txt    # é¢„å¤„ç†ä¸­æ–‡
â”œâ”€ results/
â”‚  â”œâ”€ checkpoints/                # æ¨¡å‹æƒé‡
â”‚  â”œâ”€ logs/                       # è®­ç»ƒ/è¯„ä¼°æ—¥å¿—ï¼ˆTeeé‡å®šå‘ï¼‰
â”‚  â””â”€ output/                     # æŒ‡æ ‡æ›²çº¿ã€æ ·ä¾‹CSVã€å›¾è¡¨
â”œâ”€ src/
â”‚  â”œâ”€ dataset/
â”‚  â”‚  â”œâ”€ dataloader.py            # Dataset/Collate + padding
â”‚  â”‚  â””â”€ tokenizer.py             # åˆ†è¯ä¸ Vocab æ„é€ 
â”‚  â”œâ”€ models/
â”‚  â”‚  â”œâ”€ attention.py             # Dot-Product / Multi-Head
â”‚  â”‚  â”œâ”€ encoder.py               # EncoderLayer/Encoder
â”‚  â”‚  â”œâ”€ decoder.py               # DecoderLayer/Decoder
â”‚  â”‚  â””â”€ model.py                 # Transformer
â”‚  â”œâ”€ utils/
â”‚  â”‚  â”œâ”€ logger.py                # stdout/stderr Tee
â”‚  â”‚  â””â”€ utils.py                 # æ›²çº¿ç»˜åˆ¶ã€æ¨¡å‹ç»Ÿè®¡ã€ä¿å­˜
â”‚  â”œâ”€ train.py                    # è®­ç»ƒå¾ªç¯
â”‚  â””â”€ evaluate.py                 # éªŒè¯ä¸å¯¼å‡º Top-10 æ ·ä¾‹
â”œâ”€ main.py                        # å…¥å£ï¼šé…ç½®/æ„å»º/è®­ç»ƒ/è¯„ä¼°
â”œâ”€ requirements.txt
â””â”€ README.md
```

## ğŸ”§ æ•°æ®å‡†å¤‡

é¡¹ç›®ä½¿ç”¨åˆ¶è¡¨ç¬¦åˆ†éš”çš„åŸå§‹åŒè¯­æ–‡ä»¶ data/en-cn/cmn.txtï¼ˆè‹±æ–‡ \ ä¸­æ–‡ï¼‰ã€‚é¢„å¤„ç†æ­¥éª¤å°†å…¶æ‹†åˆ†ä¸ºä¸¤ä»½ç‹¬ç«‹è¯­æ–™(å·²å®Œæˆ)ï¼š

data/en-cn/english_sentences.txt

data/en-cn/chinese_sentences.txt

ç›¸åº”è„šæœ¬å‘½ä»¤ï¼š

```bash
python src/dataset/preprocess.py --data_path data/en-cn/cmn.txt
```

## ğŸš€ å¿«é€Ÿå¼€å§‹ï¼ˆè®­ç»ƒ & è¯„ä¼°ï¼‰

```bash
python main.py
```

### ğŸ“Š è®­ç»ƒ/éªŒè¯æ›²çº¿ (Loss & Accuracy)

<p align="center">
  <img src="Transformer/results/output/train_loss_curve.png" alt="Train Loss" width="48%">
  <img src="Transformer/results/output/train_accuracy_curve.png"  alt="Train Acc"  width="48%"><br>
  <img src="Transformer/results/output/eval_loss_curve.png"   alt="Val Loss"   width="48%">
  <img src="Transformer/results/output/eval_accuracy_curve.png"    alt="Val Acc"    width="48%">
</p>

### ğŸ§ª æŒ‡æ ‡å¯¹æ¯”ï¼šä¸åŒæ³¨æ„åŠ›å¤´æ•°
| Heads | Train Loss â†“ | Val Loss â†“ | Train Acc â†‘ | Val Acc â†‘ |
|:----:|:-------------:|:----------:|:-----------:|:---------:|
| 8    | 0.081         | 0.081      | 0.519       | 0.320     |
| 16   | **0.077**     | **0.077**  | **0.520**   | **0.325** |

### ğŸ”§ ä½ç½®ç¼–ç æ¶ˆè
| Setting      | Train Loss â†“ | Val Loss â†“ | Train Acc â†‘ | Val Acc â†‘ |
|:----:|:-------------:|:----------:|:-----------:|:---------:|
| No PE | 0.082        | 0.082      | 0.517       | 0.309     |
| With PE | **0.081**    | **0.081**  | **0.519**       | **0.320**     |
### ğŸ“š æ ·ä¾‹ç¿»è¯‘ Top-10ï¼ˆENâ†’ZHï¼‰
<details>
<summary>ç‚¹å‡»å±•å¼€æŸ¥çœ‹è¡¨æ ¼</summary>

| src (EN) | trg (ZH) | pred (ZH) | acc |
|:----:|:-------------:|:----------:|:-----------:|
| "When will you come back?" "It all depends on the weather." | â€œ ä½  ä»€ ä¹ˆ æ—¶ å€™ å› æ¥ ï¼Ÿ â€ â€œ è¿™ éƒ½ è¦ çœ‹ å¤© æ°” ã€‚ â€ | â€œ ä½  ä»€ ä¹ˆ æ—¶ å€™ å› æ¥ ï¼Ÿ â€ â€œ è¿™ éƒ½ è¦ çœ‹ å¤© æ°£ ã€‚ â€ | 20 |
| The man you saw yesterday was my uncle. | ä½  æ˜¨ å¤© çœ‹ åˆ° çš„ é‚£ ä¸ª ç”· äºº æ˜¯ æˆ‘ å” å” ã€‚ | ä½  æ˜¨ å¤© çœ‹ åˆ° çš„ é‚£ ä¸ª ç”· äºº æ˜¯ æˆ‘ å” å” ã€‚ ã€‚ | 16 |
| "Our children like dogs, but I prefer cats." | æˆ‘ ä»¬ çš„ å­© å­ å–œ æ¬¢ ç‹— ï¼Œ ä½† æˆ‘ æ›´ å–œ æ¬¢ çŒ« ã€‚ | æˆ‘ ä»¬ çš„ å­© å­ å–œ æ­¡ ç‹— ï¼Œ ä½† æˆ‘ æ›´ å–œ æ¬¢ çŒ« ã€‚ ã€‚ ç€ ç€ | 16 |
| I wish we had won the game. | è¦ æ˜¯ æˆ‘ å€‘ è´ äº† é€™ å ´ æ¯” è³½ å°± å¥½ äº† ã€‚ | ä½† æ˜¯ æˆ‘ å€‘ è´ äº† é€™ å ´ æ¯” è³½ å°± å¥½ äº† ã€‚ ã€‚ ä¸­ ä¸­ ä¸­ ä¸­ ä¸­ ä¸­ ä¸­ ä¸­ ä¸­ ä¸­ ä¸­ ä¸­ ä¸­ | 14 |
| He grew up to be an engineer. | ä»– é•· å¤§ å¾Œ æˆ ç‚º äº† ä¸€ å å·¥ ç¨‹ å¸« ã€‚ | ä»– é•· å¤§ å¾Œ æˆ ç‚º äº† ä¸€ å å·¥ ç¨‹ å¸« ã€‚ ã€‚ ã€‚ ã€‚ | 14 |
| "If you want to go, then go. If you don't want to, then it's no big deal." | å¦‚ æœ ä½  æƒ³ å» ï¼Œ å°± å» å¥½ äº† ã€‚ å¦‚ æœ ä½  ä¸ æƒ³ å» ï¼Œ é‚£ ä¹Ÿ æ²¡ ä»€ ä¹ˆ å¤§ ä¸ äº† çš„ ã€‚ | å¦‚ æœ ä½  ä¸ ï¼Œ ï¼Œ é‚£ æ²’ äº† åƒ ã€‚ ä½• ä½  ä¸ æƒ³ å» ã€‚ é‚£ ä»– ä¸ æœ‰ ä¹ˆ ã€‚ èƒ½ ã€‚ ã€‚ | 13 |
| I'll give you anything that you want. | æˆ‘ æœƒ çµ¦ ä½  ä»» ä½• ä½  æƒ³ è¦ çš„ æ± è¥¿ ã€‚ | æˆ‘ æœƒ çµ¦ ä½  ä»» ä½• ä½  æƒ³ è¦ æ± æ± è¥¿ ã€‚ ã€‚ ã€‚ ã€‚ | 13 |
| I don't know if he can come tonight. | æˆ‘ ä¸ çŸ¥ é“ ä»– ä»Š æ™š æœƒ ä¸ æœƒ ä¾† ã€‚ | æˆ‘ ä¸ çŸ¥ é“ ä»– ä»Š æ™š æœƒ ä¸ æœƒ ä¾† ã€‚ ã€‚ ã€‚ ã€‚ ã€‚ ã€‚ ã€‚ ã€‚ ã€‚ | 13 |
| "The phone is ringing." "I'll get it." | â€œ ç”µ è¯ å“ äº† ã€‚ â€ â€œ æˆ‘ å» æ¥ ã€‚ â€ | â€œ ç”µ è¯ å“ äº† ã€‚ â€ â€œ æˆ‘ æ¥ æ¥ ã€‚ â€ ã€‚ åœ° åœ° åœ° åœ° åœ° åœ° | 13 |
| Tom doesn't know where Mary lives. | æ±¤ å§† ä¸ çŸ¥ é“ ç› ä¸½ ä½ åœ¨ å“ª é‡Œ ã€‚ | æ±¤ å§† ä¸ çŸ¥ é“ ç› ä¸½ ä½ åœ¨ å“ª é‡Œ ã€‚ ã€‚ ã€‚ ã€‚ ã€‚ ã€‚ ã€‚ ã€‚ ã€‚ | 13 |

</details>


## ğŸ™ è‡´è°¢ Acknowledgements

æœ¬é¡¹ç›®å‚è€ƒ/å€Ÿé‰´äº†ä»¥ä¸‹èµ„æºï¼ˆæ’åä¸åˆ†å…ˆåï¼‰ï¼š

### ğŸ“˜ è®ºæ–‡ä¸æ•™ç¨‹
- Vaswani, A., et al. *Attention Is All You Need*. 2017.  
  <https://arxiv.org/abs/1706.03762>
- Harvard NLP: *The Annotated Transformer*  
  <http://nlp.seas.harvard.edu/2018/04/03/attention.html>

### ğŸ§© å¼€æºå®ç°ä¸ç¤ºä¾‹ä»£ç 
- graykode/nlp-tutorial Â· 5-1.Transformerï¼ˆä»é›¶å®ç° Transformer çš„ç®€æ´ç¤ºä¾‹ï¼‰  
  <https://github.com/graykode/nlp-tutorial/tree/master/5-1.Transformer>
- open-source-toolkit/744aeï¼ˆéƒ¨åˆ†å®ç°æ€è·¯ä¸æ³¨é‡Šé£æ ¼å‚è€ƒï¼‰  
  <https://gitcode.com/open-source-toolkit/744ae>
- magarn/Foundations-and-Applications-of-LLM-ARC-transformerï¼ˆç›¸å…³å·¥ç¨‹ç»“æ„/å®è·µå‚è€ƒï¼‰  
  <https://github.com/magarn/Foundations-and-Applications-of-LLM-ARC-transformer>

### âœï¸ åšå®¢/ç¬”è®°
- CSDN æ–‡ç« ï¼ˆæ¦‚å¿µæ¢³ç†ä¸å®ç°è¦ç‚¹å‚è€ƒï¼‰  
  <https://blog.csdn.net/weixin_45956028/article/details/142673835>
- åšå®¢å›­ Â· limingqiï¼ˆTransformer æ¶æ„ä¸å®ç°è¦ç‚¹æ•´ç†ï¼‰  
  <https://www.cnblogs.com/limingqi/p/18992275>

### ğŸ“š å®˜æ–¹æ–‡æ¡£
- PyTorch `nn.MultiheadAttention` / `nn.Transformer` æ–‡æ¡£  
  <https://pytorch.org/docs/stable/nn.html#multiheadattention>  
  <https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html>